# RAG Practice: Node.js PDF Assistant

This project demonstrates a simple yet effective **Retrieval-Augmented Generation (RAG)** pipeline built with **LangChain**. It allows users to query a PDF document (specifically `nodejs.pdf`) and receive accurate, context-aware answers generated by OpenAI's GPT models.

## üöÄ Features

- **Document Ingestion**: Loads and processes PDF files using `PyPDFLoader`.
- **Text Chunking**: Splits large documents into manageable sections using `RecursiveCharacterTextSplitter`.
- **Vector Storage**: Uses **Qdrant** as the vector database for efficient similarity search.
- **Embeddings**: Leverages OpenAI's `text-embedding-3-small` for semantic representation.
- **Context-Aware Q&A**: Employs OpenAI's `gpt-3.5-turbo` to answer questions based strictly on the retrieved context.

## üõ†Ô∏è Tech Stack

- **Orchestration**: [LangChain](https://www.langchain.com/)
- **LLM & Embeddings**: [OpenAI](https://openai.com/)
- **Vector Database**: [Qdrant](https://qdrant.tech/)
- **Containerization**: Docker (via `docker-compose.db.yml`)
- **Language**: Python

## üèÉ Prerequisites

- Python 3.10+
- Docker & Docker Compose
- OpenAI API Key

## ‚öôÔ∏è Setup & Installation

1.  **Clone the repository**:
    ```bash
    git clone <repository-url>
    cd RAG_Practice
    ```

2.  **Install dependencies**:
    ```bash
    pip install -r requirements.txt
    ```
    *(Note: If requirements.txt is missing, install: `langchain-community langchain-openai langchain-qdrant langchain-text-splitters python-dotenv pypdf qdrant-client`)*

3.  **Configure Environment Variables**:
    Create a `.env` file in the root directory and add your OpenAI API key:
    ```env
    OPENAI_API_KEY=your_openai_api_key_here
    ```

4.  **Start the Vector Database**:
    Launch Qdrant using Docker Compose:
    ```bash
    docker-compose -f docker-compose.db.yml up -d
    ```

## üìñ Usage

### 1. Ingestion (One-time)
Before querying, you need to ingest the PDF data into Qdrant. In `index.py`, uncomment the ingestion block (lines 37-43) for the first run:

```python
# vector_store = QdrantVectorStore.from_documents(
#     documents=split_docs,
#     url="http://localhost:6333",
#     collection_name="learning_langchain",
#     embedding=embedder,
# )
```

### 2. Running the Assistant
Once ingested, you can query the system by running:
```bash
python index.py
```

By default, the script queries: *"What is FS module?"*

## üìÇ Project Structure

- `index.py`: The main script containing the RAG logic.
- `Workflows.ipynb`: Jupyter notebook for experimentation and workflow visualization.
- `docker-compose.db.yml`: Configuration to run the Qdrant database.
- `nodejs.pdf`: The source document used for the knowledge base.
- `.env`: Environment variables (API keys).

## üìÑ License
[MIT License](LICENSE.md) (or specify your license here)
